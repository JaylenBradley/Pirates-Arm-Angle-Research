Command for running ViTPose with MMDET:
python demo/top_down_img_demo_with_mmdet.py \
demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py \
https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
configs/wholebody/2d_kpt_sview_rgb_img/topdown_heatmap/coco-wholebody/hrnet_w48_coco_wholebody_384x288_dark_plus.py \
https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_wholebody_384x288_dark-f5726563_20200918.pth \
--img-root tests/data/baseball/arm/"specific video" \
--img "specific frame from video" \
--out-img-root vis_results/baseball/"specific video" \
--device=cpu
--arm=left


FFmpeg command for getting frames:
ffmpeg -i video_name.mp4 baseball_frames/frame_%04d.png


Calculate an arm angle over 10-20 videos
- Mean Absolute Error


top_down_img_demo_with_mmdet.py holds model outputs


Using COCO-WholeBody model, each person has 133 keypoints in this order:
Body (0-16):
0: nose
1: left_eye, 2: right_eye
3: left_ear, 4: right_ear
5: left_shoulder, 6: right_shoulder
7: left_elbow, 8: right_elbow
9: left_wrist, 10: right_wrist
11: left_hip, 12: right_hip
13: left_knee, 14: right_knee
15: left_ankle, 16: right_ankle
Feet (17-22): left/right foot keypoints
Face (23-90): 68 facial landmarks
Left Hand (91-111): 21 hand keypoints
Right Hand (112-132): 21 hand keypoints
Each keypoint is [x, y, confidence] where:
x, y: pixel coordinates in the original image
confidence: 0.0 to 1.0 (higher = more confident)


**calculate angle with respect to x-axis -> x-axis (1,0) * (x,y) where x is the vector from elbow to wrist and y is the vector from shoulder to elbow
**plot the vector x+y on the image as well (sum vector) and plot the x-plane vector -> debug with the vectors and angle calculation
    - plot the estimate angle and the angle that goes with it (estimate & true value) -> plot specifically the pitcher
        - calculate error between estimate & true value -> use mean absolute error or check within a window (accuracy within degree of window, then get percent of correct predictions)
        - either use the best frame or the average of the frames per video
    - quantify how often the model doesnt detect the pitcher (how sensitive is it the frame choice -> whats the max can you be off by out of a 5 frame window

    - calculate error, number of frames within the error, variance of the error -> everything comes from the intuition you gain form looking at the output
